{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - What are the prerequisites for this course?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers==2.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (4.42.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (2.3.1+cpu)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.10.13/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (0.23.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.10/site-packages (from sentence_transformers==2.7.0) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers==2.7.0) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers==2.7.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers==2.7.0) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers==2.7.0) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers==2.7.0) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers==2.7.0) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.7.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.7.0) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers==2.7.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers==2.7.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.078222655\n"
     ]
    }
   ],
   "source": [
    "user_question = \"I just discovered the course. Can I still join it?\"\n",
    "embed = model.encode(user_question)\n",
    "print(embed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.encode(\"I just discovered the course. Can I still join it?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/documents-with-ids.json'\n",
    "docs_url = f'{base_url}/{relative_url}?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = [doc for doc in documents if doc['course'] == 'machine-learning-zoomcamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings = []\n",
    "for doc in fil:\n",
    "    qa_text = f\"{doc['question']} {doc['text']}\"\n",
    "    embedding = model.encode(qa_text)\n",
    "    doc['text_vector'] = embedding.tolist()\n",
    "    embeddings.append(embedding)\n",
    "X = np.array(embeddings)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6506573\n"
     ]
    }
   ],
   "source": [
    "scores = X.dot(embed)\n",
    "print(scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VectorSearchEngine:\n",
    "    def __init__(self, documents, embeddings):\n",
    "        self.documents = documents\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def search(self, v_query, num_results=10):\n",
    "        scores = self.embeddings.dot(v_query)\n",
    "        idx = np.argsort(-scores)[:num_results]\n",
    "        return [self.documents[i] for i in idx]\n",
    "search_engine = VectorSearchEngine(documents=fil, embeddings=X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ground truth records: [{'question': 'Where can I sign up for the course?', 'course': 'machine-learning-zoomcamp', 'document': '0227b872'}, {'question': 'Can you provide a link to sign up?', 'course': 'machine-learning-zoomcamp', 'document': '0227b872'}, {'question': 'Is there an FAQ for this Machine Learning course?', 'course': 'machine-learning-zoomcamp', 'document': '0227b872'}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ground_truth_url = f'{base_url}/03-vector-search/eval/ground-truth-data.csv?raw=1'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "print(\"Sample ground truth records:\", ground_truth[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9398907103825137\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "for item in ground_truth:\n",
    "    query_embedding = model.encode(item['question'])\n",
    "    results = search_engine.search(query_embedding, num_results=5)\n",
    "    if any('id' in result and result['id'] == item['document'] for result in results):\n",
    "        hits += 1\n",
    "\n",
    "hitrate = hits / len(ground_truth)\n",
    "print(hitrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in /usr/local/python/3.10.13/lib/python3.10/site-packages (8.14.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from elasticsearch) (8.13.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.0.7)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "\n",
    "# Define the index settings and mappings\n",
    "index_name = \"course-questions\"\n",
    "index_settings = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"text_vector\": {\"type\": \"dense_vector\", \"dims\": 768}  # Adjust dimensions as needed\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index if it doesn't exist\n",
    "if not es_client.indices.exists(index=index_name):\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "# Index the documents\n",
    "for doc in documents:\n",
    "    es_client.index(index=index_name, id=doc['id'], body=doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of the document with the highest score: ee58a693\n"
     ]
    }
   ],
   "source": [
    "def elastic_search(query, index_name=\"course-questions\"):\n",
    "    search_query = {\n",
    "        \"size\": 1,  # Get the top result\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    top_result = response['hits']['hits'][0]  # Get the top result\n",
    "    return top_result['_id'], top_result['_score']  # Return the ID and score of the top document\n",
    "\n",
    "user_query = \"I just discovered the course. Can I still join it?\"\n",
    "top_result_id, top_result_score = elastic_search(user_query)\n",
    "print(f\"ID of the document with the highest score: {top_result_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query, index_name=\"course-questions\"):\n",
    "    search_query = {\n",
    "        \"size\": 5,  # Get the top 5 results\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    return result_docs  # Return the list of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit-rate for Elasticsearch: 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ground truth dataset\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth_url = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "def calculate_hit_rate_es():\n",
    "    hits = 0\n",
    "    for item in ground_truth:\n",
    "        query = item['question']\n",
    "        result_docs = elastic_search(query, index_name=\"course-questions\")\n",
    "        if any(result['id'] == item['document'] for result in result_docs):  # Compare with ground truth\n",
    "            hits += 1\n",
    "\n",
    "    hitrate = hits / len(ground_truth)\n",
    "    return hitrate\n",
    "\n",
    "hitrate_es = calculate_hit_rate_es()\n",
    "print(f\"Hit-rate for Elasticsearch: {hitrate_es:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
